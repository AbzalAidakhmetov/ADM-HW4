{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chi2_contingency\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "from collections import *\n",
    "import collections\n",
    "import heapq\n",
    "\n",
    "#!pip install prince\n",
    "\n",
    "from prince import FAMD\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark import SparkContext\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recommendation system \n",
    "Implementing a recommendation system is critical for businesses and digital platforms that want to thrive in today's competitive environment. These systems use data-driven personalization to tailor content, products, and services to individual user preferences. The latter improves user engagement, satisfaction, retention, and revenue through increased sales and cross-selling opportunities. In this section, you will attempt to implement a recommendation system by identifying similar users' preferences and recommending movies they watch to the study user. \n",
    "\n",
    "To be more specific, you will implement your version of the [**LSH algorithm**](https://www.learndatasci.com/tutorials/building-recommendation-engine-locality-sensitive-hashing-lsh-python/), which will take as input the user's preferred genre of movies, find the most similar users to this user, and recommend the most watched movies by those who are more similar to the user. \n",
    "\n",
    "__Data__: The data you will be working with can be found [here](https://www.kaggle.com/datasets/vodclickstream/netflix-audience-behaviour-uk-movies).\n",
    "\n",
    "Looking at the data, you can see that there is data available for each user for the movies the user <ins>clicked on</ins>. Gather the __title and genre__ of the __maximum top 10 movies__ that each user clicked on regarding the __number of clicks__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/theballer/Desktop/Sapienza Courses/ADM/ADM-HW4-Dataset/vodclickstream_uk_movies_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>duration</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58773</td>\n",
       "      <td>2017-01-01 01:15:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Angus, Thongs and Perfect Snogging</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2008-07-25</td>\n",
       "      <td>26bd5987e8</td>\n",
       "      <td>1dea19f6fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58774</td>\n",
       "      <td>2017-01-01 13:56:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Curse of Sleeping Beauty</td>\n",
       "      <td>Fantasy, Horror, Mystery, Thriller</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>f26ed2675e</td>\n",
       "      <td>544dcbc510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58775</td>\n",
       "      <td>2017-01-01 15:17:47</td>\n",
       "      <td>10530.0</td>\n",
       "      <td>London Has Fallen</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>f77e500e7a</td>\n",
       "      <td>7cbcc791bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58776</td>\n",
       "      <td>2017-01-01 16:04:13</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Vendetta</td>\n",
       "      <td>Action, Drama</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>c74aec7673</td>\n",
       "      <td>ebf43c36b6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58777</td>\n",
       "      <td>2017-01-01 19:16:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The SpongeBob SquarePants Movie</td>\n",
       "      <td>Animation, Action, Adventure, Comedy, Family, ...</td>\n",
       "      <td>2004-11-19</td>\n",
       "      <td>a80d6fc2aa</td>\n",
       "      <td>a57c992287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             datetime  duration  \\\n",
       "0       58773  2017-01-01 01:15:09       0.0   \n",
       "1       58774  2017-01-01 13:56:02       0.0   \n",
       "2       58775  2017-01-01 15:17:47   10530.0   \n",
       "3       58776  2017-01-01 16:04:13      49.0   \n",
       "4       58777  2017-01-01 19:16:37       0.0   \n",
       "\n",
       "                                title  \\\n",
       "0  Angus, Thongs and Perfect Snogging   \n",
       "1        The Curse of Sleeping Beauty   \n",
       "2                   London Has Fallen   \n",
       "3                            Vendetta   \n",
       "4     The SpongeBob SquarePants Movie   \n",
       "\n",
       "                                              genres release_date    movie_id  \\\n",
       "0                             Comedy, Drama, Romance   2008-07-25  26bd5987e8   \n",
       "1                 Fantasy, Horror, Mystery, Thriller   2016-06-02  f26ed2675e   \n",
       "2                                   Action, Thriller   2016-03-04  f77e500e7a   \n",
       "3                                      Action, Drama   2015-06-12  c74aec7673   \n",
       "4  Animation, Action, Adventure, Comedy, Family, ...   2004-11-19  a80d6fc2aa   \n",
       "\n",
       "      user_id  \n",
       "0  1dea19f6fe  \n",
       "1  544dcbc510  \n",
       "2  7cbcc791bf  \n",
       "3  ebf43c36b6  \n",
       "4  a57c992287  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top movies clicked by the users in total.\n",
    "\n",
    "So, here I have grouped by title and genres first and used count method, after I sorted by duration, which is basically sorting by count number, not by the values of duration itself, because I used group by on them all the columns will have the count number, instead of regular numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black Mirror: Bandersnatch</td>\n",
       "      <td>Drama, Mystery, Sci-Fi, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bright</td>\n",
       "      <td>Action, Fantasy, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avengers: Age of Ultron</td>\n",
       "      <td>Action, Adventure, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annihilation</td>\n",
       "      <td>Adventure, Drama, Horror, Mystery, Sci-Fi, Thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hot Fuzz</td>\n",
       "      <td>Action, Comedy, Mystery, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deadpool</td>\n",
       "      <td>Action, Adventure, Comedy, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bird Box</td>\n",
       "      <td>Drama, Horror, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FYRE: The Greatest Party That Never Happened</td>\n",
       "      <td>Documentary, Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Big Short</td>\n",
       "      <td>Biography, Comedy, Drama, History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hitman's Bodyguard</td>\n",
       "      <td>Action, Comedy, Crime, Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                    Black Mirror: Bandersnatch   \n",
       "1                                        Bright   \n",
       "2                       Avengers: Age of Ultron   \n",
       "3                                  Annihilation   \n",
       "4                                      Hot Fuzz   \n",
       "5                                      Deadpool   \n",
       "6                                      Bird Box   \n",
       "7  FYRE: The Greatest Party That Never Happened   \n",
       "8                                 The Big Short   \n",
       "9                        The Hitman's Bodyguard   \n",
       "\n",
       "                                              genres  \n",
       "0                   Drama, Mystery, Sci-Fi, Thriller  \n",
       "1                          Action, Fantasy, Thriller  \n",
       "2                          Action, Adventure, Sci-Fi  \n",
       "3  Adventure, Drama, Horror, Mystery, Sci-Fi, Thr...  \n",
       "4                  Action, Comedy, Mystery, Thriller  \n",
       "5                  Action, Adventure, Comedy, Sci-Fi  \n",
       "6                              Drama, Horror, Sci-Fi  \n",
       "7                                 Documentary, Music  \n",
       "8                  Biography, Comedy, Drama, History  \n",
       "9                    Action, Comedy, Crime, Thriller  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['title', 'genres']).count().sort_values(by='duration', ascending=False).reset_index()[['title', 'genres']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Minhash Signatures \n",
    "Using the movie genre and user_ids, try to implement your min-hash signatures so that users with similar interests in a genre appear in the same bucket. \n",
    "\n",
    "__Important note:__ You must write your minhash function from scratch.  You are not permitted to use any already implemented hash functions.  Read the class materials and, if necessary, conduct an internet search.  The description of hash functions in the [book](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf) may be helpful as a reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>duration</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58773</td>\n",
       "      <td>2017-01-01 01:15:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Angus, Thongs and Perfect Snogging</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2008-07-25</td>\n",
       "      <td>26bd5987e8</td>\n",
       "      <td>1dea19f6fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58774</td>\n",
       "      <td>2017-01-01 13:56:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Curse of Sleeping Beauty</td>\n",
       "      <td>Fantasy, Horror, Mystery, Thriller</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>f26ed2675e</td>\n",
       "      <td>544dcbc510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58775</td>\n",
       "      <td>2017-01-01 15:17:47</td>\n",
       "      <td>10530.0</td>\n",
       "      <td>London Has Fallen</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>f77e500e7a</td>\n",
       "      <td>7cbcc791bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58776</td>\n",
       "      <td>2017-01-01 16:04:13</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Vendetta</td>\n",
       "      <td>Action, Drama</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>c74aec7673</td>\n",
       "      <td>ebf43c36b6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58777</td>\n",
       "      <td>2017-01-01 19:16:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The SpongeBob SquarePants Movie</td>\n",
       "      <td>Animation, Action, Adventure, Comedy, Family, ...</td>\n",
       "      <td>2004-11-19</td>\n",
       "      <td>a80d6fc2aa</td>\n",
       "      <td>a57c992287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             datetime  duration  \\\n",
       "0       58773  2017-01-01 01:15:09       0.0   \n",
       "1       58774  2017-01-01 13:56:02       0.0   \n",
       "2       58775  2017-01-01 15:17:47   10530.0   \n",
       "3       58776  2017-01-01 16:04:13      49.0   \n",
       "4       58777  2017-01-01 19:16:37       0.0   \n",
       "\n",
       "                                title  \\\n",
       "0  Angus, Thongs and Perfect Snogging   \n",
       "1        The Curse of Sleeping Beauty   \n",
       "2                   London Has Fallen   \n",
       "3                            Vendetta   \n",
       "4     The SpongeBob SquarePants Movie   \n",
       "\n",
       "                                              genres release_date    movie_id  \\\n",
       "0                             Comedy, Drama, Romance   2008-07-25  26bd5987e8   \n",
       "1                 Fantasy, Horror, Mystery, Thriller   2016-06-02  f26ed2675e   \n",
       "2                                   Action, Thriller   2016-03-04  f77e500e7a   \n",
       "3                                      Action, Drama   2015-06-12  c74aec7673   \n",
       "4  Animation, Action, Adventure, Comedy, Family, ...   2004-11-19  a80d6fc2aa   \n",
       "\n",
       "      user_id  \n",
       "0  1dea19f6fe  \n",
       "1  544dcbc510  \n",
       "2  7cbcc791bf  \n",
       "3  ebf43c36b6  \n",
       "4  a57c992287  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea for solving \n",
    "\n",
    "First of all, let's create a dataframe with all genres that have been watched by unique users. So, we can use it for further calculations and for creation of a signature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lsh will contain only user_id and genres\n",
    "df_lsh = df.loc[:,['user_id', 'genres']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "#drop potential null values from the description column\n",
    "df_lsh = df_lsh.dropna(subset=['genres'])\n",
    "#uses apply method with list comprehension to tokenize each row and stem each genre\n",
    "df_lsh['genres_clean'] = df_lsh.genres.apply(lambda row: [stemmer.stem(word) for word in nltk.word_tokenize(row)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed that there are some redundant words and symbols, which I will take away\n",
    "remove_words = [',', 'avail', 'not']\n",
    "df_lsh.genres_clean = df_lsh.genres_clean.apply(lambda row: [word for word in row if word not in remove_words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           [None, None, None]\n",
       "1                     [None, None, None, None]\n",
       "2                                 [None, None]\n",
       "3                                 [None, None]\n",
       "4         [None, None, None, None, None, None]\n",
       "                          ...                 \n",
       "671731                                  [None]\n",
       "671732          [None, None, None, None, None]\n",
       "671733                      [None, None, None]\n",
       "671734                            [None, None]\n",
       "671735                            [None, None]\n",
       "Name: genres_clean, Length: 671736, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add unique genres to out vocabulary set\n",
    "vocabulary = set()\n",
    "df_lsh.genres_clean.apply(lambda row: [vocabulary.add(word) for word in row]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>genres</th>\n",
       "      <th>genres_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1dea19f6fe</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>[comedi, drama, romanc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544dcbc510</td>\n",
       "      <td>Fantasy, Horror, Mystery, Thriller</td>\n",
       "      <td>[fantasi, horror, mysteri, thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7cbcc791bf</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>[action, thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebf43c36b6</td>\n",
       "      <td>Action, Drama</td>\n",
       "      <td>[action, drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a57c992287</td>\n",
       "      <td>Animation, Action, Adventure, Comedy, Family, ...</td>\n",
       "      <td>[anim, action, adventur, comedi, famili, fantasi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                             genres  \\\n",
       "0  1dea19f6fe                             Comedy, Drama, Romance   \n",
       "1  544dcbc510                 Fantasy, Horror, Mystery, Thriller   \n",
       "2  7cbcc791bf                                   Action, Thriller   \n",
       "3  ebf43c36b6                                      Action, Drama   \n",
       "4  a57c992287  Animation, Action, Adventure, Comedy, Family, ...   \n",
       "\n",
       "                                        genres_clean  \n",
       "0                            [comedi, drama, romanc]  \n",
       "1               [fantasi, horror, mysteri, thriller]  \n",
       "2                                 [action, thriller]  \n",
       "3                                    [action, drama]  \n",
       "4  [anim, action, adventur, comedi, famili, fantasi]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lsh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice\n",
    "\n",
    "In the above dataframe we already have a list of genres that user watched, but they are not unique, thus, let us use grouby with sum, so the lists of the same users will be added together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsh = df_lsh.groupby(by='user_id').agg({'genres_clean': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove repeated values from the lists just by converting them to set\n",
    "df_lsh.genres_clean = df_lsh.genres_clean.apply(lambda row: set(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsh = df_lsh.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert vocabulary and the watched genres by the users again to the list, so we can easily work further\n",
    "vocabulary = list(vocabulary)\n",
    "df_lsh.genres_clean = df_lsh.genres_clean.apply(lambda row: list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>genres_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00004e2862</td>\n",
       "      <td>[crime, drama, thriller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>[comedi, horror, drama, mysteri, fantasi, adve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000090e7c8</td>\n",
       "      <td>[mysteri, thriller, sci-fi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000118a755</td>\n",
       "      <td>[horror]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000296842d</td>\n",
       "      <td>[mysteri, thriller, sci-fi, drama]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                       genres_clean\n",
       "0  00004e2862                           [crime, drama, thriller]\n",
       "1  000052a0a0  [comedi, horror, drama, mysteri, fantasi, adve...\n",
       "2  000090e7c8                        [mysteri, thriller, sci-fi]\n",
       "3  000118a755                                           [horror]\n",
       "4  000296842d                 [mysteri, thriller, sci-fi, drama]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the final df_lsh, that has unique user ids with all the genres that was watched by the particular user.\n",
    "df_lsh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Signature Matrix\n",
    "\n",
    "In order to create signature matrix, first we have to create shingles, the index of which will be hashed for creation of the signature matrix. So, in case of some sentences we could have used shingles of two characters, however, here we have specific information, which are genres. Thus, I have decided to use genres as my shingles. \n",
    "\n",
    "Let's use dictionary comprehension for creation of shingles with unique identifiers, they will serve as the row ids that will be hashed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingle_dict = {genre: i for i, genre in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comedi': 0,\n",
       " 'drama': 1,\n",
       " 'fantasi': 2,\n",
       " 'adventur': 3,\n",
       " 'film-noir': 4,\n",
       " 'sci-fi': 5,\n",
       " 'reality-tv': 6,\n",
       " 'anim': 7,\n",
       " 'mysteri': 8,\n",
       " 'sport': 9,\n",
       " 'thriller': 10,\n",
       " 'crime': 11,\n",
       " 'music': 12,\n",
       " 'action': 13,\n",
       " 'short': 14,\n",
       " 'documentari': 15,\n",
       " 'horror': 16,\n",
       " 'war': 17,\n",
       " 'western': 18,\n",
       " 'famili': 19,\n",
       " 'talk-show': 20,\n",
       " 'new': 21,\n",
       " 'romanc': 22,\n",
       " 'biographi': 23,\n",
       " 'histori': 24}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shingle_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "In order to create a signature matrix, we do not need to use the one hot encoding vectores with the genres that every user has watched and permute them further. This is too time consuming. So, the idea is that we really should avoid matrix with most of the values that are zeros. We only care about the values that are ones. \n",
    "\n",
    "Therefore, we use shingle dict that was created above as the true row ids, then let's say that some particular user is watching thriller, sport and crime. We know their true row ids are 24, 23 and 20 respectively. We will use one has function for all users for creating the one row of the signature matrix, hence, we will hash those values of true row ids and take the minimum output from three values and put it to the signature matrix. \n",
    "\n",
    "Now, as the idea of using hash functions and shingle dict is clear, let's explain my approach. I will use 10 different hash functions to create 10 outputs for some particular user's genre, update the signature matrix and go to the next genre of that user and so on untill I finish with one user. This, then iteratively performed on all other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(shingle_dict)\n",
    "n_sig = 10\n",
    "# Our hash will be (a*x + b) % N, where a and b are random numbers in range of N\n",
    "# Here params will be a and b for all 10 hash functions\n",
    "params = np.random.randint(N, size=[n_sig,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this hash function, to hash the row ids\n",
    "# The hash function is basically (a*x + b) % N\n",
    "def _permuteRow(row):\n",
    "    return (params@np.array([1, row]))%N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a signature matrix with all values as infinity, we make them infinity, \n",
    "# so they will be changed with the outputs from the hash functions \n",
    "# notice that in our signature matrix, columns are users and the rows are the outputs from the unique\n",
    "# hash functions\n",
    "sig = np.full((n_sig, df_lsh.shape[0]), np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the core in creation of the signature matrix, it will go through our previously created \n",
    "# df_lsh, for each user it will update the values in signature matrix, leaving the minimum one. \n",
    "for j, row in df_lsh.iterrows():\n",
    "    for shingle in row['genres_clean']:\n",
    "        orig_row = shingle_dict[shingle]\n",
    "        curr_col = _permuteRow(orig_row)\n",
    "        sig[:,j] = np.minimum(sig[:,j],curr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42936/1918318916.py:2: RuntimeWarning: invalid value encountered in cast\n",
      "  sig = sig.astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>161908</th>\n",
       "      <th>161909</th>\n",
       "      <th>161910</th>\n",
       "      <th>161911</th>\n",
       "      <th>161912</th>\n",
       "      <th>161913</th>\n",
       "      <th>161914</th>\n",
       "      <th>161915</th>\n",
       "      <th>161916</th>\n",
       "      <th>161917</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 161918 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0       4       0       2       9       2       4       4       2       1   \n",
       "1       5       0       0       1       0      11      11       0       2   \n",
       "2       4       0      11       9      11       1      19      13       0   \n",
       "3      15       0      13       0      13       3       2       3       1   \n",
       "4       0       0      16      10       0       0       0       1       1   \n",
       "5       4       0       4      12       4      13      17      14       0   \n",
       "6       0       0       0      14       0       4       3       5      10   \n",
       "7       4       2       5      24       5       4      14      10       8   \n",
       "8       1       1       1      15       1       5       4       6      11   \n",
       "9       0       0       0       7       0       2       2       5       5   \n",
       "\n",
       "   9       ...  161908  161909  161910  161911  161912  161913  161914  \\\n",
       "0       2  ...       0       2       3       4      14      13       2   \n",
       "1       0  ...      10       0      18       2      16      10       0   \n",
       "2       6  ...       1       0       2       0      24       3      18   \n",
       "3       3  ...       2       3       1       2       5       8      13   \n",
       "4       1  ...       0       1       1       0      20      11       0   \n",
       "5       3  ...      11       3       3       4       7      18       4   \n",
       "6       5  ...       3       2      10       0      24      12       0   \n",
       "7       2  ...       0       2      12       2       9       0       5   \n",
       "8       6  ...       4       3      11       1       0      13       1   \n",
       "9       5  ...       2       1       5       0      12       6       0   \n",
       "\n",
       "   161915  161916  161917  \n",
       "0       0      12       4  \n",
       "1       4       3      11  \n",
       "2       1       3       4  \n",
       "3       3       3      15  \n",
       "4       0       1       0  \n",
       "5       5       4      17  \n",
       "6       4       0       4  \n",
       "7       0       0       4  \n",
       "8       5       1       5  \n",
       "9       2       0       2  \n",
       "\n",
       "[10 rows x 161918 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all the values in signature matrix to integer type\n",
    "sig = sig.astype(int)\n",
    "# Let's visualize the signature matrix\n",
    "pd.DataFrame(sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "Now that your buckets are ready, it's time to ask a few queries. We will provide you with some user_ids and ask you to recommend at __most five movies__ to the user to watch based on the movies clicked by similar users. \n",
    "\n",
    "To recommend at most five movies given a user_id, use the following procedure: \n",
    "\n",
    "1. Identify the <ins>two most similar</ins> users to this user.\n",
    "2. If these two users have any movies __in common__, recommend those movies based on the total number of clicks by these users.\n",
    "3. If there are __no more common__ movies, try to propose the most clicked movies by the __most similar user first__, followed by the other user. \n",
    "\n",
    "__Note:__ At the end of the process, we expect to see at most five movies recommended to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used two sources from the internet, which were really useful. They can be found [here](https://towardsdatascience.com/locality-sensitive-hashing-how-to-find-similar-items-in-a-large-set-with-precision-d907c52b05fc) and [here](https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach for creating buckets\n",
    "\n",
    "Basically, the following lines of code below, put similar candidates to the same bucket. This is achieved using defaultdict with set values. We start going through columns which are unique users in some particular band, and our keys in defaultdict are column numbers from the signature matrix. Thus, we try to maximize the number of colissions, is two exactly similar columns are encountered they are certainly put to the same bucket, in out case, buckets are simply sets. Defaultdict is used for the case if there is no any key with that value, it will create one without throwing an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def fastCandidatePairs(sig_mat, b, r):\n",
    "    n, d = sig_mat.shape\n",
    "    \n",
    "    hashbuckets = collections.defaultdict(set)\n",
    "    bands = np.array_split(sig_mat, b, axis=0)\n",
    "    for i, band in enumerate(bands):\n",
    "        for j in range(d):\n",
    "            # The last value made a string to prevent colissions between bands\n",
    "            band_id = tuple(list(band[:,j])+[str(i)])\n",
    "            hashbuckets[band_id].add(j)\n",
    "    bucket_candidates = list()\n",
    "    for bucket in hashbuckets.values():\n",
    "        if len(bucket) > 1:\n",
    "            bucket_candidates.append(bucket)\n",
    "    return bucket_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 161918)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = fastCandidatePairs(sig, b=2, r=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing band `b` and row `r`\n",
    "\n",
    "To choose `b` and `r`, we have to lean to the formula from the theory:\n",
    "\n",
    "The threshold \\( t \\) for the Locality-Sensitive Hashing (LSH) theorem, with respect to bands \\( b \\) and rows \\( r \\), is given by:\n",
    "\n",
    "$$\n",
    "t = (1/b)^{1/r}\n",
    "$$\n",
    "\n",
    "Where b times r  is equal to the total number of hash functions used.\n",
    "In our case, when \\( b = 2 \\) bands and \\( r = 5 \\) rows, the threshold \\( t \\) would be:\n",
    "\n",
    "$$\n",
    "t = (1/2)^{1/5} \\approx 0.8706 \n",
    "$$\n",
    "\n",
    "This value of \\( t \\) determines the similarity threshold for hashing to the same bucket in at least one of the bands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After getting our buckets, we need to some score to evaluate which user is more similar than other one\n",
    "# Here, I am using simple Jaccard Similarity, which is basically intersection over union.\n",
    "def score(target_user_id, current_user_id, df):\n",
    "    current_user = df.genres_clean.iloc[current_user_id]\n",
    "    target_user = df.genres_clean.iloc[target_user_id]\n",
    "    return len(set(target_user).intersection(set(current_user)))/len(set(target_user).union(set(current_user)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(candidate_pairs, query_results, user_ids, df):\n",
    "    # iterate over given user_ids\n",
    "    for user_id in user_ids:\n",
    "        # I need to match user_ids to indecies of my df_lsh, which I used to hash and \n",
    "        # perfrom other operations\n",
    "        target_user_id = df.loc[df.user_id == user_id].index[0]\n",
    "        # declare max_heap for keepint top-2 scores \n",
    "        max_heap = []\n",
    "        # Iterate through buckets \n",
    "        for bucket in candidate_pairs:\n",
    "            # Go further if there is out target user for whom we are looking two most similar candidates\n",
    "            if target_user_id in bucket:\n",
    "\n",
    "                # Iterate through users\n",
    "                for current_user_id in bucket:\n",
    "                    # Check if our user is not a target user, if not go further\n",
    "                    if current_user_id != target_user_id:\n",
    "                        # get a score\n",
    "                        current_score = score(target_user_id, current_user_id, df)\n",
    "                        # push the tuple consisting of current score and user_id\n",
    "                        # check if the (current_score, current_user_id) is not already in the max_heap\n",
    "                        # which could be the case, because we have not only one band\n",
    "                        if (current_score, current_user_id) not in max_heap:\n",
    "                            heapq.heappush(max_heap, (current_score, current_user_id))\n",
    "                        if len(max_heap) > 2:\n",
    "                            # remove the minimum if the length of \n",
    "                            # the heap larger than 2\n",
    "                            heapq.heappop(max_heap)\n",
    "        # put the two most simlar candidates after going through all buckets\n",
    "        # it is important because target users could be in other buckets too!\n",
    "        # and our job is to find two most similar candidates across all buckets\n",
    "        query_results[target_user_id] = max_heap\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for finding two most similar users\n",
    "\n",
    "Below, few lines of the code will go through some dummy user_ids and will recommend movies for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ad08fad2ec'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take some user_ids\n",
    "df.user_id[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = [df.user_id[100], df.user_id[140000], df.user_id[51478]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_results = query(candidate_pairs, {}, user_ids, df_lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{109317: [(1.0, 160034), (1.0, 160180)],\n",
       " 89168: [(0.8181818181818182, 41311), (0.8181818181818182, 136981)],\n",
       " 121616: [(1.0, 155637), (1.0, 157204)]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Recommendations\n",
    "\n",
    "After acquiring our results as a dictionary with keys as a target users for whom we are finding similar users and values as two tuples with the score and row id of the corresponding user we have to make recommendations for each user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create an empy dataframe where we will put our recommendations for each user\n",
    "recommendations_df = pd.DataFrame(columns=['user_id', 'recommended_movies'])\n",
    "\n",
    "# Go through each user\n",
    "for user_id in user_ids:\n",
    "    recommended_movies = []\n",
    "\n",
    "    # Identify two most similar users\n",
    "    (_, second), (_, first) = lsh_results[df_lsh.loc[df_lsh.user_id == user_id].index[0]]\n",
    "    \n",
    "    # Let's find the movies that were already watched by the target user,\n",
    "    # to not accidentally reccommend those movies\n",
    "    df_target_movies = set(df.loc[df.user_id == user_id].title)\n",
    "    # DataFrames for the first and second users\n",
    "    df_first = df.loc[df.user_id == df_lsh.user_id.iloc[first]]\n",
    "    df_second = df.loc[df.user_id == df_lsh.user_id.iloc[second]]\n",
    "\n",
    "    # Sets of movies for each user\n",
    "    movies_first = set(df_first.title)\n",
    "    movies_second = set(df_second.title)\n",
    "\n",
    "    # Find common movies\n",
    "    common_movies = movies_first.intersection(movies_second)\n",
    "    common_movies = [movie for movie in common_movies if movie not in df_target_movies]\n",
    "\n",
    "    # Go further if there are some common movies\n",
    "    if common_movies:\n",
    "        # concatenate df_second under df_first\n",
    "        combined_df = pd.concat([df_first, df_second])\n",
    "        # filter by common movies\n",
    "        combined_df = combined_df[combined_df.title.isin(common_movies)]\n",
    "        # count the number of times each title was watched and sort them in descending order\n",
    "        combined_df = combined_df.groupby('title').agg({'user_id': 'count'}).reset_index().sort_values(by='user_id', ascending=False)\n",
    "        # add those movies \n",
    "        recommended_movies.extend(combined_df.title.tolist())\n",
    "    if len(recommended_movies) >= 5:\n",
    "        recommended_movies = recommended_movies[:6]\n",
    "    else:\n",
    "        # Add movies from each user if needed\n",
    "        for df_user in [df_first, df_second]:\n",
    "            if len(recommended_movies) < 5:\n",
    "                top_movies = df_user.groupby('title').agg({'user_id':'count'}).reset_index().sort_values(by='user_id', ascending=False)\n",
    "                for movie in top_movies.title:\n",
    "                    # check some edge cases\n",
    "                    if len(recommended_movies) < 5 and movie not in recommended_movies and movie not in df_target_movies:\n",
    "                        recommended_movies.append(movie)\n",
    "    \n",
    "    new_row = pd.DataFrame({'user_id': [user_id], 'recommended_movies': [recommended_movies]})\n",
    "    recommendations_df = pd.concat([recommendations_df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing results\n",
    "\n",
    "Below, we have a dataframe with the recommendation movies for each of the target user. \n",
    "Let's try to analyze our results, and see if the recommendations are legitimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recommended_movies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad08fad2ec</td>\n",
       "      <td>[The Spy Who Dumped Me, Role Models]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8d438090c0</td>\n",
       "      <td>[Alice Through the Looking Glass, Ella Enchant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c078c3bb09</td>\n",
       "      <td>[Before I Fall, The Overnight, Here Alone, Lov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                 recommended_movies\n",
       "0  ad08fad2ec               [The Spy Who Dumped Me, Role Models]\n",
       "1  8d438090c0  [Alice Through the Looking Glass, Ella Enchant...\n",
       "2  c078c3bb09  [Before I Fall, The Overnight, Here Alone, Lov..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>duration</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134067</th>\n",
       "      <td>192840</td>\n",
       "      <td>2017-07-29 15:42:36</td>\n",
       "      <td>103533.0</td>\n",
       "      <td>The Incredible Jessica James</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>f4f5186800</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134842</th>\n",
       "      <td>193615</td>\n",
       "      <td>2017-07-30 20:28:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Handsome Devil</td>\n",
       "      <td>Comedy, Drama, Sport</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>cf85ceead2</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134963</th>\n",
       "      <td>193736</td>\n",
       "      <td>2017-07-30 22:56:56</td>\n",
       "      <td>9784.0</td>\n",
       "      <td>Le Week-End</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2013-10-11</td>\n",
       "      <td>50605a2238</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135488</th>\n",
       "      <td>194261</td>\n",
       "      <td>2017-07-31 01:40:00</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>The Accidental Husband</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>2008-02-29</td>\n",
       "      <td>12d726e81f</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135639</th>\n",
       "      <td>194412</td>\n",
       "      <td>2017-07-31 18:54:51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>An Unfinished Life</td>\n",
       "      <td>Drama, Family, Romance</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>fe0a7072f2</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135652</th>\n",
       "      <td>194425</td>\n",
       "      <td>2017-07-31 02:59:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Les Misérables</td>\n",
       "      <td>Drama, History, Musical, Romance, War</td>\n",
       "      <td>2012-12-25</td>\n",
       "      <td>99e99b992b</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135729</th>\n",
       "      <td>194502</td>\n",
       "      <td>2017-07-31 03:09:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chef</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>2014-05-30</td>\n",
       "      <td>3ba55497d3</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136047</th>\n",
       "      <td>194820</td>\n",
       "      <td>2017-08-01 15:00:27</td>\n",
       "      <td>11115.0</td>\n",
       "      <td>The Virgin Suicides</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>2000-05-19</td>\n",
       "      <td>ef19c6dbea</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138338</th>\n",
       "      <td>197111</td>\n",
       "      <td>2017-08-04 12:01:16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dallas Buyers Club</td>\n",
       "      <td>Biography, Drama</td>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>f28dd5526d</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>198773</td>\n",
       "      <td>2017-08-06 19:02:20</td>\n",
       "      <td>5258.0</td>\n",
       "      <td>Be Somebody</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2016-06-10</td>\n",
       "      <td>6550a34aec</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140167</th>\n",
       "      <td>198940</td>\n",
       "      <td>2017-08-07 15:36:24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trainwreck</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>230728d966</td>\n",
       "      <td>8d438090c0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0             datetime  duration  \\\n",
       "134067      192840  2017-07-29 15:42:36  103533.0   \n",
       "134842      193615  2017-07-30 20:28:09       0.0   \n",
       "134963      193736  2017-07-30 22:56:56    9784.0   \n",
       "135488      194261  2017-07-31 01:40:00    4750.0   \n",
       "135639      194412  2017-07-31 18:54:51       0.0   \n",
       "135652      194425  2017-07-31 02:59:10       0.0   \n",
       "135729      194502  2017-07-31 03:09:10       0.0   \n",
       "136047      194820  2017-08-01 15:00:27   11115.0   \n",
       "138338      197111  2017-08-04 12:01:16       0.0   \n",
       "140000      198773  2017-08-06 19:02:20    5258.0   \n",
       "140167      198940  2017-08-07 15:36:24       0.0   \n",
       "\n",
       "                               title                                 genres  \\\n",
       "134067  The Incredible Jessica James                                 Comedy   \n",
       "134842                Handsome Devil                   Comedy, Drama, Sport   \n",
       "134963                   Le Week-End                 Comedy, Drama, Romance   \n",
       "135488        The Accidental Husband                        Comedy, Romance   \n",
       "135639            An Unfinished Life                 Drama, Family, Romance   \n",
       "135652                Les Misérables  Drama, History, Musical, Romance, War   \n",
       "135729                          Chef               Adventure, Comedy, Drama   \n",
       "136047           The Virgin Suicides                         Drama, Romance   \n",
       "138338            Dallas Buyers Club                       Biography, Drama   \n",
       "140000                   Be Somebody                 Comedy, Drama, Romance   \n",
       "140167                    Trainwreck                 Comedy, Drama, Romance   \n",
       "\n",
       "       release_date    movie_id     user_id  \n",
       "134067   2017-07-28  f4f5186800  8d438090c0  \n",
       "134842   2017-06-02  cf85ceead2  8d438090c0  \n",
       "134963   2013-10-11  50605a2238  8d438090c0  \n",
       "135488   2008-02-29  12d726e81f  8d438090c0  \n",
       "135639   2005-09-16  fe0a7072f2  8d438090c0  \n",
       "135652   2012-12-25  99e99b992b  8d438090c0  \n",
       "135729   2014-05-30  3ba55497d3  8d438090c0  \n",
       "136047   2000-05-19  ef19c6dbea  8d438090c0  \n",
       "138338   2013-11-22  f28dd5526d  8d438090c0  \n",
       "140000   2016-06-10  6550a34aec  8d438090c0  \n",
       "140167   2015-07-17  230728d966  8d438090c0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.user_id == \"8d438090c0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of movies recommended for the user 8d438090c0\n",
    "movies = recommendations_df[recommendations_df.user_id == \"8d438090c0\"].recommended_movies.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Christmas Prince</td>\n",
       "      <td>Comedy, Family, Romance</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Christmas Prince</td>\n",
       "      <td>NOT AVAILABLE</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice Through the Looking Glass</td>\n",
       "      <td>Adventure, Family, Fantasy</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ella Enchanted</td>\n",
       "      <td>Comedy, Family, Fantasy, Romance</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One Day</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Patch Adams</td>\n",
       "      <td>Biography, Comedy, Drama, Romance</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title                             genres  count\n",
       "0               A Christmas Prince            Comedy, Family, Romance    668\n",
       "1               A Christmas Prince                      NOT AVAILABLE     34\n",
       "2  Alice Through the Looking Glass         Adventure, Family, Fantasy    269\n",
       "3                   Ella Enchanted   Comedy, Family, Fantasy, Romance    259\n",
       "4                          One Day                     Drama, Romance    145\n",
       "5                      Patch Adams  Biography, Comedy, Drama, Romance     71"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.title.isin(movies)].groupby(by=['title', 'genres']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "As it can be seen from the dataframes above, the recommended movies make sense, they match on genres and also none of the recommended movies was watched by the target user, which is good. Overall, LSH algorithm was successfully implemented, even though the number of signatures were not so big, because of the limited number of genres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grouping Users together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will deal with clustering algorithms that will provide groups of Netflix users that are similar among them.\n",
    "\n",
    "To solve this task, you must accomplish the following stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data + feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Access to the data found in this dataset\n",
    "\n",
    "2.Sometimes, the features (variables, fields) are not given in a dataset but can be created from it; this is known as feature engineering. For example, the original dataset has several clicks done by the same user, so grouping data by user_id will allow you to create new features for each user:\n",
    "\n",
    "a) Favorite genre (i.e., the genre on which the user spent the most time)\n",
    "\n",
    "b) Average click duration\n",
    "\n",
    "c) Time of the day (Morning/Afternoon/Night) when the user spends the most time on the platform (the time spent is tracked through the duration of the clicks)\n",
    "\n",
    "d) Is the user an old movie lover, or is he into more recent stuff (content released after 2010)?\n",
    "\n",
    "e) Average time spent a day by the user (considering only the days he logs in)\n",
    "\n",
    "So, in the end, you should have for each user_id five features.\n",
    "\n",
    "3.Consider at least 10 additional features that can be generated for each user_id (you can use chatGPT or other LLM tools for suggesting features to create). Describe each of them and add them to the previous dataset you made (the one with five features). In the end, you should have for each user at least 15 features (5 recommended + 10 suggested by you)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us first visualize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"vodclickstream_uk_movies_03.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.datetime = pd.to_datetime(df.datetime, errors='coerce')\n",
    "df.release_date = pd.to_datetime(df.release_date, errors='coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.genres = df.genres.apply(lambda row: row.split(', '))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are required to create features such as Favorite genre, Average click duration,  Time of the day, Is the user an old movie lover, Average time spent a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty user dataframe\n",
    "user_data = pd.DataFrame(df['user_id'].unique(), columns=[\"user_id\"])\n",
    "\n",
    "# Feature 1 Favorite genre \n",
    "genre_data = df.explode(\"genres\")[[\"user_id\", \"genres\", \"duration\"]]\n",
    "genre_data = genre_data.groupby([\"user_id\", \"genres\"]).sum()\n",
    "genre_data = genre_data.groupby(\"user_id\").idxmax()\n",
    "genre_data.duration = genre_data.duration.apply(lambda row: row[1])\n",
    "genre_data = genre_data.rename(columns={'duration': 'favorite_genre'})\n",
    "user_data = user_data.merge(genre_data, left_on=\"user_id\", right_index=True)\n",
    "\n",
    "# Feature 2 Average click duration\n",
    "avg_duration_data = df.groupby(\"user_id\").duration.mean().reset_index()\n",
    "avg_duration_data = avg_duration_data.rename(columns={'duration': 'average_click_duration'})\n",
    "user_data = user_data.merge(avg_duration_data)\n",
    "\n",
    "# Feature 3 Time of the day when the user spends the most time\n",
    "time_data = df[[\"user_id\", \"datetime\", \"duration\"]].copy()\n",
    "time_data['datetime'] = time_data['datetime'].dt.hour\n",
    "time_data['datetime'] = time_data['datetime'].apply(lambda x: 'morning' if 4 <= x < 12 else 'afternoon' if 12 <= x < 20 else 'night')\n",
    "time_data = time_data.groupby([\"user_id\", \"datetime\"]).sum().groupby(\"user_id\").idxmax()\n",
    "time_data.duration = time_data.duration.apply(lambda row: row[1])\n",
    "time_data = time_data.rename(columns={'duration': 'most_active_time_of_day'})\n",
    "user_data = user_data.merge(time_data, left_on=\"user_id\", right_index=True)\n",
    "\n",
    "# Feature 4 Is the user an old movie lover \n",
    "movie_data = df[[\"user_id\", \"release_date\"]].copy()\n",
    "movie_data.release_date = movie_data.release_date.dt.year\n",
    "movie_data[\"new\"] = movie_data.release_date > 2010\n",
    "movie_data[\"old\"] = movie_data.release_date <= 2010\n",
    "user_oldnew_data = movie_data.groupby(\"user_id\").old.sum() > movie_data.groupby(\"user_id\").new.sum()\n",
    "user_oldnew_data.name = \"is_oldmovies_lover\"\n",
    "user_data = user_data.merge(user_oldnew_data, left_on=\"user_id\", right_index=True)\n",
    "\n",
    "# Feature 5 Average time spent a day by the user\n",
    "days_data = df[[\"user_id\", \"datetime\"]].copy()\n",
    "days_data[\"datetime\"] = days_data[\"datetime\"].dt.floor('D')\n",
    "days_data = days_data.groupby(\"user_id\").datetime.nunique()\n",
    "sums_data = df.groupby(\"user_id\").duration.sum()\n",
    "avg_per_day_data = sums_data / days_data\n",
    "avg_per_day_data.name = \"average_time_per_day\"\n",
    "user_data = user_data.merge(avg_per_day_data, left_on=\"user_id\", right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are required to create 10 more features. We selected, \n",
    "6. Number of active days \n",
    "7. Weekend watcher\n",
    "8. Genre diversity\n",
    "9. Number of unique movies \n",
    "10. Highest number of rewatches \n",
    "11. Favorite day of the week \n",
    "12. Duration of the longest movie for each user\n",
    "13. Longest gap between clicks\n",
    "14. Late night watcher\n",
    "15. Most common release year for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 6 number of active days \n",
    "days_active = df[[\"user_id\", \"datetime\"]].copy()\n",
    "days_active[\"datetime\"] = days_active[\"datetime\"].dt.floor('D')\n",
    "days_active = days_active.groupby(\"user_id\").datetime.nunique()\n",
    "days_active.name = \"active_days\"\n",
    "\n",
    "user_data = user_data.merge(days_active, left_on = \"user_id\", right_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 7 Weekend watcher\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "\n",
    "# Classify days as weekend (5-Saturday, 6-Sunday) or not\n",
    "df['weekend_watcher'] = df['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Get the user's weekend watching behavior\n",
    "weekend_watcher = df.groupby('user_id')['weekend_watcher'].any().reset_index()\n",
    "weekend_watcher.rename(columns={'weekend_watcher': 'weekend_watcher'}, inplace=True)\n",
    "user_data = user_data.merge(weekend_watcher, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 8 Genre diversity\n",
    "exploded_df = df.explode('genres')\n",
    "\n",
    "# Count the number of unique genres for each user\n",
    "genre_diversity = exploded_df.groupby('user_id')['genres'].nunique().reset_index()\n",
    "genre_diversity.rename(columns={'genres': 'genre_diversity'}, inplace=True)\n",
    "\n",
    "# Merge into the existing DataFrame (assuming 'user_data' is your existing DataFrame)\n",
    "user_data = user_data.merge(genre_diversity, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 9 Number of unique movies \n",
    "n_movies = df.groupby(\"user_id\").movie_id.nunique()\n",
    "n_movies.name = \"n_movies\"\n",
    "user_data = user_data.merge(n_movies, left_on = \"user_id\", right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 10 Highest number of rewatches \n",
    "rewatch = df.groupby([\"user_id\", \"movie_id\"]).size().groupby(\"user_id\").max()\n",
    "rewatch.name = \"max_rewatches\"\n",
    "\n",
    "user_data = user_data.merge(rewatch, left_on = \"user_id\", right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 11 Favorite day of the week \n",
    "favourite_day = df[[\"user_id\", \"datetime\"]].copy()\n",
    "favourite_day.datetime = favourite_day.datetime.dt.dayofweek\n",
    "favourite_day = favourite_day.groupby([\"user_id\", \"datetime\"]).size().groupby(\"user_id\").idxmax().apply(lambda x: x[1])\n",
    "favourite_day.name = \"favorite_day\"\n",
    "\n",
    "user_data = user_data.merge(favourite_day, left_on = \"user_id\", right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 12 Duration of the longest movie for each user\n",
    "longest_movie_duration = df.groupby('user_id')['duration'].max().reset_index()\n",
    "longest_movie_duration.rename(columns={'duration': 'longest_movie_duration'}, inplace=True)\n",
    "user_data = user_data.merge(longest_movie_duration, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 13 Longest gap between clicks\n",
    "df_sorted = df.sort_values(['user_id', 'datetime'])\n",
    "\n",
    "# Calculate the time difference between consecutive clicks for each user\n",
    "df_sorted['time_diff'] = df_sorted.groupby('user_id')['datetime'].diff()\n",
    "\n",
    "# Calculate the longest time gap between clicks for each user\n",
    "longest_gap_between_clicks = df_sorted.groupby('user_id')['time_diff'].max().reset_index()\n",
    "longest_gap_between_clicks.rename(columns={'time_diff': 'longest_gap_between_clicks'}, inplace=True)\n",
    "\n",
    "# Merge into the existing DataFrame\n",
    "user_data = user_data.merge(longest_gap_between_clicks, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 14 Late night watcher\n",
    "df['late_night_watcher'] = df['datetime'].dt.hour.between(0, 6)\n",
    "\n",
    "# Get the user's late-night watching behavior\n",
    "late_night_watcher = df.groupby('user_id')['late_night_watcher'].any().reset_index()\n",
    "late_night_watcher.rename(columns={'late_night_watcher': 'late_night_watcher'}, inplace=True)\n",
    "user_data = user_data.merge(late_night_watcher, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 15 Most common release year for each user\n",
    "popular_release_year = df.groupby('user_id')['release_date'].agg(lambda x: x.mode().dt.year.iat[0] if not x.mode().empty else None).reset_index()\n",
    "popular_release_year.rename(columns={'release_date': 'popular_release_year'}, inplace=True)\n",
    "user_data = user_data.merge(popular_release_year, on='user_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So the new dataframe is following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that you have plenty of features to work with now. So, it would be best to find a way to reduce the dimensionality (reduce the number of variables to work with). You can follow the subsequent directions to achieve it:\n",
    "\n",
    "1.To normalise or not to normalise? That's the question. Sometimes, it is worth normalizing (scaling) the features. Explain if it is a good idea to perform any normalization method. If you think the normalization should be used, apply it to your data (look at the available normalization functions in the scikit-learn library).\n",
    "\n",
    "2.Select one method for dimensionality reduction and apply it to your data. Some suggestions are Principal Component Analysis, Multiple Correspondence Analysis, Singular Value Decomposition, Factor Analysis for Mixed Data, Two-Steps clustering. Make sure that the method you choose applies to the features you have or modify your data to be able to use it. Explain why you chose that method and the limitations it may have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalizing numerical features is essential to ensure that features with different scales do not disproportionately influence machine learning models. It helps in achieving a consistent scale across all features, preventing the dominance of certain features and ensuring that the model's performance is not adversely affected by varying magnitudes of numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us normilize the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for normalization (excluding non-numeric columns)\n",
    "features_to_normalize = user_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the selected features\n",
    "user_data[features_to_normalize] = scaler.fit_transform(user_data[features_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data  #normalized the numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We should eliminate the NaN values and duplicating values to have proper data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all NaN values of the normalized dataframe\n",
    "user_data_norm = user_data.dropna()\n",
    "\n",
    "# Group the dataframe by 'user_id' and all the extracted features\n",
    "user_data_grouped = user_data_norm[['user_id','favorite_genre', 'average_click_duration', 'most_active_time_of_day', 'is_oldmovies_lover',\n",
    "                      'average_time_per_day', 'active_days', 'weekend_watcher', 'genre_diversity', 'n_movies',\n",
    "                      'max_rewatches', 'favorite_day', 'longest_movie_duration', 'longest_gap_between_clicks',\n",
    "                      'late_night_watcher', 'popular_release_year']]\n",
    "\n",
    "# Drop duplicate values\n",
    "user_data_grouped = user_data_grouped.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We chose Factor Analysis of Mixed Data (FAMD) for dimensionality reduction because it effectively handles a mix of numerical and categorical features in user preferences. However, FAMD assumes linear relationships and may not capture complex nonlinear patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We selected following features to work in FAMD, 'favorite_genre', 'is_oldmovies_lover', 'average_time_per_day', 'n_movies', 'max_rewatches', 'late_night_watcher', 'popular_release_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The selected features for FAMD provide a well-rounded representation of user behavior and preferences on the platform. 'Favorite_genre' and 'is_oldmovies_lover' capture individual taste and inclination towards classic films. 'Average_time_per_day' and 'n_movies' quantify user engagement, while 'max_rewatches' indicates specific movie preferences. 'Late_night_watcher' adds temporal insights, and 'popular_release_year' gauges the influence of movie release years on user interactions. Together, these features offer a nuanced understanding of user interactions, allowing for comprehensive analysis and informed decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for FAMD, both numerical and categorical\n",
    "famd_features = ['favorite_genre', 'is_oldmovies_lover',\n",
    "                  'average_time_per_day', 'n_movies',\n",
    "                  'max_rewatches',\n",
    "                  'late_night_watcher', 'popular_release_year']\n",
    "\n",
    "user_data_famd = user_data_grouped[famd_features]\n",
    "\n",
    "# Apply FAMD\n",
    "famd_model = FAMD(n_components=6, n_iter=15, random_state=10)\n",
    "user_data_famd_result = famd_model.fit_transform(user_data_famd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_model.eigenvalues_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking the eigenvalues in FAMD is crucial as they indicate the amount of variance explained by each principal component. Examining the eigenvalues helps identify the components that contribute significantly to the data variability, guiding the selection of an appropriate number of dimensions to retain for meaningful interpretation and analysis. High variance suggests that the feature associated with that component contributes significantly to the overall diversity of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Implement the K-means clustering algorithm (not ++: random initialization) using MapReduce. We ask you to write the algorithm from scratch following what you learned in class.\n",
    "\n",
    "2.Find an optimal number of clusters. Use at least two different methods. If your algorithms provide diverse optimal K's, select one of them and explain why you chose it.\n",
    "\n",
    "3.Run the algorithm on the data obtained from the dimensionality reduction.\n",
    "\n",
    "4.Implement K-means++ from scratch and explain the differences with the results you got earlier.\n",
    "\n",
    "5.Ask ChatGPT to recommend other clustering algorithms and choose one. Explain your choice, then ask ChatGPT to implement it or use already implemented versions (e.g., the one provided in the scikit-learn library) and run it on your data. Explain the differences (if there are any) in the results. Which one is the best, in your opinion, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us implement the K-means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_kmeans(data_point, centroids):\n",
    "    # Find the nearest cluster center for the data point\n",
    "    nearest_center = np.argmin(np.linalg.norm(centroids - data_point, axis=1))\n",
    "    \n",
    "    # Emit the cluster index and the data point\n",
    "    yield nearest_center, data_point.tolist()\n",
    "\n",
    "def reduce_kmeans(cluster_index, data_points):\n",
    "    # Calculate the mean to obtain new centroid\n",
    "    new_centroid = np.mean(data_points, axis=0)\n",
    "    \n",
    "    # Emit the cluster index and the new centroid\n",
    "    yield cluster_index, new_centroid.tolist()\n",
    "    \n",
    "def group_by_key(mapped_data):\n",
    "    grouped_data = {}\n",
    "    for key, value in mapped_data:\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "        grouped_data[key].append(value)\n",
    "    return grouped_data.items()    \n",
    "\n",
    "def kmeans_map_reduce(data, k, max_iterations):\n",
    "    # Randomly select initial cluster representatives\n",
    "    initial_centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n",
    "\n",
    "    # Execute a fixed number of iterations\n",
    "    for _ in range(max_iterations):\n",
    "        # Map step: Assign each data point to the nearest centroid\n",
    "        mapped_data = []\n",
    "        for data_point in data:\n",
    "            mapped_data.extend(map_kmeans(data_point, initial_centroids))\n",
    "\n",
    "        # Reduce step: Calculate the mean to obtain new centroids\n",
    "        reduced_data = {}\n",
    "        for cluster_index, data_points in group_by_key(mapped_data):\n",
    "            reduced_data.update(reduce_kmeans(cluster_index, data_points))\n",
    "\n",
    "        # Update centroids\n",
    "        new_centroids = np.array(list(reduced_data.values()))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(new_centroids, initial_centroids, rtol=0.1):\n",
    "            print(\"Convergence found!\")\n",
    "            break\n",
    "\n",
    "        # Update initial centroids\n",
    "        initial_centroids = new_centroids\n",
    "\n",
    "    return initial_centroids\n",
    "\n",
    "\n",
    "famd_data = user_data_famd_result.values\n",
    "num_clusters = 3\n",
    "max_iterations = 10\n",
    "final_centroids = kmeans_map_reduce(famd_data, num_clusters, max_iterations)\n",
    "print(\"Final Centroids:\", final_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming famd_data is your feature matrix\n",
    "famd_data = user_data_famd_result.values\n",
    "\n",
    "# Try different values of k\n",
    "k_values = range(2, 5)\n",
    "inertia_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(famd_data)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Elbow Method is a heuristic used to determine the optimal number of clusters (k) by analyzing the inertia values. In your case, the plot shows that the inertia decreases and then the rate of decrease slows down, forming an \"elbow\" at k=3. The point where the inertia starts to level off is considered a suitable choice for the number of clusters, and in this scenario, k=3 was chosen to strike a balance between reducing inertia and avoiding excessive complexity in the clustering.\n",
    "\n",
    "* So optimal number of clusters is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimal k based on the Elbow Method \n",
    "optimal_k =3\n",
    "\n",
    "# Apply K-means clustering with the optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(famd_data)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(famd_data[:, 0], famd_data[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='X', s=200, c='black', label='Centroids')\n",
    "plt.xlabel('FAMD Component 0')\n",
    "plt.ylabel('FAMD Component 1')\n",
    "plt.title(f'K-means Clustering (k={optimal_k})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us implement K-means++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plusplus(data, k, max_iterations=100):\n",
    "    # Step 1: Randomly choose the first centroid\n",
    "    centroids = [data[np.random.choice(len(data))]]\n",
    "\n",
    "    for _ in range(k - 1):\n",
    "        # Step 2: For each data point, compute its distance to the nearest existing centroid\n",
    "        distances = np.array([min(np.linalg.norm(point - centroid) for centroid in centroids) for point in data])\n",
    "\n",
    "        # Step 3: Choose the next centroid from the data points with probability proportional to distance squared\n",
    "        probabilities = distances**2 / sum(distances**2)\n",
    "        next_centroid = data[np.random.choice(len(data), p=probabilities)]\n",
    "\n",
    "        centroids.append(next_centroid)\n",
    "\n",
    "    # Run standard K-means with the obtained initial centroids\n",
    "    return kmeans(data, np.array(centroids), max_iterations)\n",
    "\n",
    "def kmeans(data, initial_centroids, max_iterations=100):\n",
    "    centroids = initial_centroids\n",
    "    for iteration in range(max_iterations):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        labels = np.argmin(np.linalg.norm(data[:, None] - centroids, axis=2), axis=1)\n",
    "\n",
    "        # Update centroids based on the mean of data points in each cluster\n",
    "        new_centroids = np.array([data[labels == c].mean(axis=0) for c in range(len(centroids))])\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(new_centroids, centroids, rtol=0.1):\n",
    "            print(\"Convergence found!\")\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return labels, centroids\n",
    "\n",
    "# Assuming famd_data is your feature matrix\n",
    "famd_data = user_data_famd_result.values\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "k_values = range(2, 11)\n",
    "inertia_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    _, centroids = kmeans_plusplus(famd_data, k)\n",
    "    inertia_values.append(sum(np.min(np.linalg.norm(famd_data[:, None] - centroids, axis=2), axis=1)))\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k (K-means++)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the K-means++ variant, the decrease in inertia stops at k=5, that point on the Elbow curve indicates a potential optimal number of clusters. In this case, choosing k=5 aligns with the point where additional clusters contribute less to reducing the overall inertia, suggesting a meaningful clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimal k based on the Elbow Method \n",
    "optimal_k =5\n",
    "\n",
    "# Apply K-means clustering with the optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(famd_data)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(famd_data[:, 0], famd_data[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='X', s=200, c='black', label='Centroids')\n",
    "plt.xlabel('FAMD Component 0')\n",
    "plt.ylabel('FAMD Component 1')\n",
    "plt.title(f'K-means ++ Clustering (k={optimal_k})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means++ is an improvement over the standard K-means initialization by providing a more sophisticated method for selecting initial centroids. It aims to distribute the initial centroids more effectively across the data, which often leads to faster convergence and more reliable results. The primary difference lies in the initialization step, where K-means++ improves the starting positions of centroids, potentially resulting in a better final clustering outcome compared to the random initialization used in the standard K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The differences in cluster separation between K-means and K-means++ may arise due to variations in the initialization of centroids. K-means++ aims to improve initialization, but if the dataset lacks distinct cluster structures or contains outliers, the resulting clusters may still exhibit less separation, impacting the overall clustering performance as in our case. The clusters are less separated in the K-means++ scenario for our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Among other clustering algorithms ChatGPT suggested DBSCAN (Density-Based Spatial Clustering of Applications with Noise). This is a density-based clustering algorithm that groups together data points that are close to each other and marks outliers as noise. Unlike K-means, DBSCAN does not require specifying the number of clusters beforehand. It defines clusters based on the density of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming famd_data is your feature matrix\n",
    "famd_data = user_data_famd_result.values\n",
    "\n",
    "# Initialize and fit DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels_dbscan = dbscan.fit_predict(famd_data)\n",
    "\n",
    "# Plot DBSCAN results\n",
    "plt.scatter(famd_data[:, 0], famd_data[:, 1], c=labels_dbscan, cmap='viridis', alpha=0.5)\n",
    "plt.title('DBSCAN Clustering Results')\n",
    "plt.xlabel('Component 0')\n",
    "plt.ylabel('Component 1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DBSCAN is a density-based clustering algorithm that automatically identifies clusters based on data density, eliminating the need to specify the number of clusters in advance. In contrast to K-means, DBSCAN excels in handling irregularly shaped clusters and detecting outliers, offering greater flexibility for diverse datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
